{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zp\n",
    "import pandas as pd\n",
    "#from pypac import PACSession as Session #or use requests below if non-ONS\n",
    "from requests import Session\n",
    "from io import BytesIO\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Summary Prescription Data for England\n",
    "\n",
    "This code has been created to work with 2016, 2017 and 2018 data, earlier data may have format inconsistencies that need to be dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set File Location of Prescribing Data\n",
    "\n",
    "These files are the \"GP practice prescribing data - Presentation level\" zip files that come from: https://data.gov.uk/dataset/176ae264-2484-4afe-a297-d51798eb8228/gp-practice-prescribing-data-presentation-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to zip files\n",
    "# path = r\"[CHANGE THIS PATH]\\England\\\\\"\n",
    "path = r\"england\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in drug data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get drug data (NB some drugs duplicated for illnesses)\n",
    "drug_data = pd.read_csv(path + r\"/drug_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                   illness     medication\n0               depression     fluoxetine\n1               depression     citalopram\n2               depression     paroxetine\n3               depression     sertraline\n4               depression     dapoxetine\n5               depression   escitalopram\n6               depression    fluvoxamine\n7               depression     duloxetine\n8               depression    venlafaxine\n9               depression    mirtazapine\n10              depression  amitriptyline\n11              depression   clomipramine\n12              depression     imipramine\n13              depression    lofepramine\n14              depression  nortriptyline\n15              alzheimers      donepezil\n16              alzheimers   rivastigmine\n17              alzheimers    galantamine\n18          blood pressure      enalapril\n19          blood pressure     lisinopril\n20          blood pressure    perindopril\n21          blood pressure       ramipril\n22          blood pressure    candesartan\n23          blood pressure     irbesartan\n24          blood pressure       losartan\n25          blood pressure      valsartan\n26          blood pressure     olmesartan\n27          blood pressure     amlodipine\n28          blood pressure     felodipine\n29          blood pressure     nifedipine\n30            hypertension     olmesartan\n31            hypertension      verapamil\n32            hypertension    perindopril\n33            hypertension       losartan\n34            hypertension     nifedipine\n35            hypertension      enalapril\n36            hypertension     candesarta\n37            hypertension     amlodipine\n38            hypertension     lisinopril\n39            hypertension     irbesartan\n40            hypertension     felodipine\n41            hypertension       ramipril\n42            hypertension      valsartan\n43            hypertension      diltiazem\n44               diabeties      metformin\n45  cardiovascular disease   atorvastatin\n46  cardiovascular disease    fluvastatin\n47  cardiovascular disease    pravastatin\n48  cardiovascular disease   rosuvastatin\n49  cardiovascular disease    simvastatin\n50                insomnia      zopiclone\n51               addiction      methadone\n52          social anxiety   escitalopram",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>illness</th>\n      <th>medication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>depression</td>\n      <td>fluoxetine</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>depression</td>\n      <td>citalopram</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>depression</td>\n      <td>paroxetine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>depression</td>\n      <td>sertraline</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>depression</td>\n      <td>dapoxetine</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>depression</td>\n      <td>escitalopram</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>depression</td>\n      <td>fluvoxamine</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>depression</td>\n      <td>duloxetine</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>depression</td>\n      <td>venlafaxine</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>depression</td>\n      <td>mirtazapine</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>depression</td>\n      <td>amitriptyline</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>depression</td>\n      <td>clomipramine</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>depression</td>\n      <td>imipramine</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>depression</td>\n      <td>lofepramine</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>depression</td>\n      <td>nortriptyline</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>alzheimers</td>\n      <td>donepezil</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>alzheimers</td>\n      <td>rivastigmine</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>alzheimers</td>\n      <td>galantamine</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>blood pressure</td>\n      <td>enalapril</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>blood pressure</td>\n      <td>lisinopril</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>blood pressure</td>\n      <td>perindopril</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>blood pressure</td>\n      <td>ramipril</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>blood pressure</td>\n      <td>candesartan</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>blood pressure</td>\n      <td>irbesartan</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>blood pressure</td>\n      <td>losartan</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>blood pressure</td>\n      <td>valsartan</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>blood pressure</td>\n      <td>olmesartan</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>blood pressure</td>\n      <td>amlodipine</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>blood pressure</td>\n      <td>felodipine</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>blood pressure</td>\n      <td>nifedipine</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>hypertension</td>\n      <td>olmesartan</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>hypertension</td>\n      <td>verapamil</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>hypertension</td>\n      <td>perindopril</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>hypertension</td>\n      <td>losartan</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>hypertension</td>\n      <td>nifedipine</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>hypertension</td>\n      <td>enalapril</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>hypertension</td>\n      <td>candesarta</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>hypertension</td>\n      <td>amlodipine</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>hypertension</td>\n      <td>lisinopril</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>hypertension</td>\n      <td>irbesartan</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>hypertension</td>\n      <td>felodipine</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>hypertension</td>\n      <td>ramipril</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>hypertension</td>\n      <td>valsartan</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>hypertension</td>\n      <td>diltiazem</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>diabeties</td>\n      <td>metformin</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>cardiovascular disease</td>\n      <td>atorvastatin</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>cardiovascular disease</td>\n      <td>fluvastatin</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>cardiovascular disease</td>\n      <td>pravastatin</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>cardiovascular disease</td>\n      <td>rosuvastatin</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>cardiovascular disease</td>\n      <td>simvastatin</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>insomnia</td>\n      <td>zopiclone</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>addiction</td>\n      <td>methadone</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>social anxiety</td>\n      <td>escitalopram</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Prescribing Data\n",
    "\n",
    "This code iterates over the monthly prescribing data, ultimately producing an aggregate table.\n",
    "\n",
    "Note, if you want to find prescribing for non-loneliness relatived diseases, all you need to do is provide a different set of drug_data and edit the code_loneliness function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find loneliness related prescribing\n",
    "def code_loneliness(x):\n",
    "    out = {}\n",
    "    # coding by illness categories\n",
    "    for illness in drug_data['illness'].unique():\n",
    "        out[illness] = x['BNF NAME'].str.contains(\"|\".join(drug_data[drug_data['illness'] == illness]['medication']),\n",
    "                                                  case=False, \n",
    "                                                  regex=True).astype('int16')\n",
    "    # Make dataframe\n",
    "    out = pd.DataFrame(out)\n",
    "    # Add loneliness related disease binary - avoids double counting some drugs.\n",
    "    out['loneliness'] = x['BNF NAME'].str.contains(\"|\".join(drug_data['medication'].unique()),\n",
    "                                                   case = False, \n",
    "                                                   regex = True).astype('int16')\n",
    "    # Return dataframe multiplied by counts of items.\n",
    "    return out.multiply(x['ITEMS'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary for aggregation\n",
    "agg_cols = {col : 'sum' for col in drug_data['illness'].unique()}\n",
    "agg_cols['ITEMS'] = 'sum'\n",
    "agg_cols['loneliness'] = 'sum'\n",
    "for key in ['Date','SHA','PCT','pcstrip','CenterName','Street','Town','Town2','Postcode']:\n",
    "    agg_cols[key] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-206450546c85>:32: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  prescribe['pcstrip'] = prescribe['Postcode'].str.replace(\"\\s\",\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018_10_Oct.zip\n",
      "2018_08_Aug.zip\n",
      "2018_05_May.zip\n",
      "2018_07_Jul.zip\n",
      "2018_06_Jun.zip\n",
      "2018_09_Sep.zip\n",
      "2018_12_Dec.zip\n",
      "2018_11_Nov.zip\n",
      "2018_04_Apr.zip\n",
      "2018_02_Feb.zip\n",
      "2018_03_Mar.zip\n",
      "2018_01_Jan.zip\n"
     ]
    }
   ],
   "source": [
    "monthly_data = []\n",
    "\n",
    "for file in os.listdir(path + '/zip'):\n",
    "    with zp.ZipFile(path + '/zip/' + file) as zipf:\n",
    "        zip_names = zipf.namelist()\n",
    "\n",
    "        # Deal with Address Files\n",
    "        addr_name = next((filename for filename in zip_names if \"ADDR\" in filename), None)\n",
    "        # Open address file in pandas, set header.\n",
    "        addr = pd.read_csv(zipf.open(addr_name), \n",
    "                           header=0, \n",
    "                           names = [\"Date\", \"PracCode\", \"PracName\",\"CenterName\",\n",
    "                                    \"Street\", \"Town\", \"Town2\", \"Postcode\"], \n",
    "                           usecols = range(8))\n",
    "\n",
    "        # Deal with prescription info\n",
    "        prescribe_name = next((filename for filename in zip_names if \"PDPI\" in filename), None)\n",
    "        # Open prescribing files in pandas.\n",
    "        prescribe = pd.read_csv(zipf.open(prescribe_name))\n",
    "        prescribe.columns = prescribe.columns.str.strip()\n",
    "        # Get counts of prescribing dataframe for loneliness related diseases\n",
    "        loneliness_prescribing = code_loneliness(prescribe[['BNF NAME','ITEMS']])\n",
    "        # merge dataframes\n",
    "        prescribe = prescribe.merge(loneliness_prescribing, left_index=True, right_index=True)\n",
    "        del loneliness_prescribing\n",
    "        \n",
    "        # merge in address information\n",
    "        prescribe = prescribe.merge(addr, left_on = 'PRACTICE', right_on = 'PracCode')\n",
    "        del addr\n",
    "        \n",
    "        # Create uniform postcode field\n",
    "        prescribe['pcstrip'] = prescribe['Postcode'].str.replace(\"\\s\",\"\")\n",
    "        \n",
    "        # get a summary - grouping by PracCode\n",
    "        summary = prescribe.groupby('PracCode', as_index=False).agg(agg_cols)\n",
    "        del prescribe\n",
    "        \n",
    "        monthly_data.append(summary)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the monthly data together.\n",
    "data = pd.concat(monthly_data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path + \"/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Postcode Location\n",
    "\n",
    "Postcode location is pulled in from the latest ONS NSPL (National Statistics Postcode Lookup). There is a guide to fields here: http://geoportal.statistics.gov.uk/datasets/0a404beab6f544be8fb72d0c2b12d62b\n",
    "\n",
    "NB - If using an ONS laptop `pip install pypac`, if not comment pypac import above and use requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'england\\\\processed_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-83e99129ec06>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"\\processed_data.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1048\u001B[0m             )\n\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m         \"\"\"\n\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'england\\\\processed_data.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(path + \"\\processed_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/anaconda3/envs/Loneliness_study/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'geoportal.statistics.gov.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Read in postcode lookup data\n",
    "# This is the persistent link to the latest ONS NSPL\n",
    "postcode_url = \"http://geoportal.statistics.gov.uk/datasets/055c2d8135ca4297a85d624bb68aefdb_0.csv\"\n",
    "\n",
    "with Session() as session:\n",
    "    response = session.get(postcode_url, verify = False)\n",
    "\n",
    "field_dtypes = {'objectid': 'int32', 'pcd':'str', 'pcd2': 'str', 'pcds':'str', 'dointr':'str','doterm':'str',\n",
    "                'usertype':'int8','oseast1m': 'float', 'osnorth1m': 'float', 'osgrdind':'int8', 'lat':'float', \n",
    "                'long':'float', 'X':'float', 'Y':'float', 'imd': 'int8',\n",
    "                'oa11':'str', 'cty': 'str', 'ced':'str', 'laua': 'str', 'ward': 'str', 'hlthau':'str',\n",
    "                'ctry': 'str','pcon': 'str','eer': 'str','teclec': 'str','ttwa': 'str','pct': 'str','nuts': 'str',\n",
    "                'park': 'str','lsoa11': 'str','msoa11': 'str','wz11': 'str','ccg': 'str','bua11': 'str',\n",
    "                'buasd11': 'str','ru11ind': 'str','oac11': 'str','lep1': 'str','lep2': 'str','pfa': 'str',\n",
    "                'ced': 'str','nhser': 'str','rgn': 'str','calncv': 'str','stp': 'str'}\n",
    "\n",
    "#pc = pd.read_csv(BytesIO(response.content), dtype = field_dtypes)\n",
    "pc = pd.read_csv(path + '/NSPL_AUG_2018_UK.csv', dtype=field_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-f8a74b782d6c>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  pc['pcstrip'] = pc['pcd'].str.replace(\"\\s\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# create pcstrip for matching\n",
    "pc['pcstrip'] = pc['pcd'].str.replace(\"\\s\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB - here I'm joining 2011 LSOA, 2011 MSOA, Rural-Urban Indicator, Region (formally GOR), Local Authority Area (effectively district), and IMD score (NB separate for each country). However, you can add any of the geography codes available in the NSPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = data.merge(pc[['pcstrip','oseast1m','osnrth1m','lsoa11','msoa11','ru11ind','rgn','laua','imd']], \n",
    "                       how = 'left',\n",
    "                       on = 'pcstrip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "      pcstrip  Year  NUMBER_OF_PATIENTS  SHA  PCT  oseast1m_x  osnrth1m_x  \\\n0     AL100BS  2018         9624.416667  Q58  06K    522443.0    208996.0   \n1     AL100NL  2018        12741.333333  Q58  06K    522442.0    208808.0   \n2     AL108HP  2018        10664.750000  Q58  06K    522445.0    208444.0   \n3      AL13FY  2018         4034.666667  Q58  06N    515627.0    206743.0   \n4      AL13HD  2018        20919.166667  Q58  06N    515081.0    207765.0   \n...       ...   ...                 ...  ...  ...         ...         ...   \n6571   YO71LU  2018         8402.333333  Q50  03D    442965.0    481974.0   \n6572   YO73RP  2018         3124.333333  Q50  03D    440259.0    475953.0   \n6573   YO84BL  2018        10717.916667  Q50  03Q    461055.0    432478.0   \n6574   YO84QH  2018        16984.666667  Q50  03Q    461500.0    432020.0   \n6575   YO89AJ  2018        15926.083333  Q50  03Q    460694.0    431348.0   \n\n       lsoa11_x   msoa11_x ru11ind_x  ... loneliness_zscore  loneills  \\\n0     E01023927  E02004990        C1  ...         -0.001787 -0.530497   \n1     E01023920  E02004991        C1  ...         -0.216148 -0.244936   \n2     E01023922  E02004991        C1  ...          0.365776 -1.077905   \n3     E01023729  E02004935        C1  ...          2.053380 -2.078672   \n4     E01023676  E02004934        C1  ...         -0.518973  0.381142   \n...         ...        ...       ...  ...               ...       ...   \n6571  E01027630  E02005757        D2  ...         -0.328948  1.318448   \n6572  E01027635  E02005758        E1  ...          1.059750 -0.815316   \n6573  E01027908  E02005813        C1  ...         -0.741733 -2.343730   \n6574  E01027909  E02005813        C1  ...         -0.524102 -1.586530   \n6575  E01027882  E02005815        C1  ...          0.053301 -1.301268   \n\n      oseast1m_y  osnrth1m_y   lsoa11_y   msoa11_y  ru11ind_y      rgn_y  \\\n0       522443.0    208996.0  E01023927  E02004990         C1  E12000006   \n1       522442.0    208808.0  E01023920  E02004991         C1  E12000006   \n2       522445.0    208444.0  E01023922  E02004991         C1  E12000006   \n3       515627.0    206743.0  E01023729  E02004935         C1  E12000006   \n4       515081.0    207765.0  E01023676  E02004934         C1  E12000006   \n...          ...         ...        ...        ...        ...        ...   \n6571    442965.0    481974.0  E01027630  E02005757         D2  E12000003   \n6572    440259.0    475953.0  E01027635  E02005758         E1  E12000003   \n6573    461055.0    432478.0  E01027908  E02005813         C1  E12000003   \n6574    461500.0    432020.0  E01027909  E02005813         C1  E12000003   \n6575    460694.0    431348.0  E01027882  E02005815         C1  E12000003   \n\n         laua_y  imd_y  \n0     E07000241    115  \n1     E07000241    -35  \n2     E07000241     23  \n3     E07000240    -19  \n4     E07000240    107  \n...         ...    ...  \n6571  E07000164     70  \n6572  E07000164    116  \n6573  E07000169    -94  \n6574  E07000169     30  \n6575  E07000169     81  \n\n[6576 rows x 42 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pcstrip</th>\n      <th>Year</th>\n      <th>NUMBER_OF_PATIENTS</th>\n      <th>SHA</th>\n      <th>PCT</th>\n      <th>oseast1m_x</th>\n      <th>osnrth1m_x</th>\n      <th>lsoa11_x</th>\n      <th>msoa11_x</th>\n      <th>ru11ind_x</th>\n      <th>...</th>\n      <th>loneliness_zscore</th>\n      <th>loneills</th>\n      <th>oseast1m_y</th>\n      <th>osnrth1m_y</th>\n      <th>lsoa11_y</th>\n      <th>msoa11_y</th>\n      <th>ru11ind_y</th>\n      <th>rgn_y</th>\n      <th>laua_y</th>\n      <th>imd_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AL100BS</td>\n      <td>2018</td>\n      <td>9624.416667</td>\n      <td>Q58</td>\n      <td>06K</td>\n      <td>522443.0</td>\n      <td>208996.0</td>\n      <td>E01023927</td>\n      <td>E02004990</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>-0.001787</td>\n      <td>-0.530497</td>\n      <td>522443.0</td>\n      <td>208996.0</td>\n      <td>E01023927</td>\n      <td>E02004990</td>\n      <td>C1</td>\n      <td>E12000006</td>\n      <td>E07000241</td>\n      <td>115</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AL100NL</td>\n      <td>2018</td>\n      <td>12741.333333</td>\n      <td>Q58</td>\n      <td>06K</td>\n      <td>522442.0</td>\n      <td>208808.0</td>\n      <td>E01023920</td>\n      <td>E02004991</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>-0.216148</td>\n      <td>-0.244936</td>\n      <td>522442.0</td>\n      <td>208808.0</td>\n      <td>E01023920</td>\n      <td>E02004991</td>\n      <td>C1</td>\n      <td>E12000006</td>\n      <td>E07000241</td>\n      <td>-35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AL108HP</td>\n      <td>2018</td>\n      <td>10664.750000</td>\n      <td>Q58</td>\n      <td>06K</td>\n      <td>522445.0</td>\n      <td>208444.0</td>\n      <td>E01023922</td>\n      <td>E02004991</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>0.365776</td>\n      <td>-1.077905</td>\n      <td>522445.0</td>\n      <td>208444.0</td>\n      <td>E01023922</td>\n      <td>E02004991</td>\n      <td>C1</td>\n      <td>E12000006</td>\n      <td>E07000241</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AL13FY</td>\n      <td>2018</td>\n      <td>4034.666667</td>\n      <td>Q58</td>\n      <td>06N</td>\n      <td>515627.0</td>\n      <td>206743.0</td>\n      <td>E01023729</td>\n      <td>E02004935</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>2.053380</td>\n      <td>-2.078672</td>\n      <td>515627.0</td>\n      <td>206743.0</td>\n      <td>E01023729</td>\n      <td>E02004935</td>\n      <td>C1</td>\n      <td>E12000006</td>\n      <td>E07000240</td>\n      <td>-19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AL13HD</td>\n      <td>2018</td>\n      <td>20919.166667</td>\n      <td>Q58</td>\n      <td>06N</td>\n      <td>515081.0</td>\n      <td>207765.0</td>\n      <td>E01023676</td>\n      <td>E02004934</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>-0.518973</td>\n      <td>0.381142</td>\n      <td>515081.0</td>\n      <td>207765.0</td>\n      <td>E01023676</td>\n      <td>E02004934</td>\n      <td>C1</td>\n      <td>E12000006</td>\n      <td>E07000240</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6571</th>\n      <td>YO71LU</td>\n      <td>2018</td>\n      <td>8402.333333</td>\n      <td>Q50</td>\n      <td>03D</td>\n      <td>442965.0</td>\n      <td>481974.0</td>\n      <td>E01027630</td>\n      <td>E02005757</td>\n      <td>D2</td>\n      <td>...</td>\n      <td>-0.328948</td>\n      <td>1.318448</td>\n      <td>442965.0</td>\n      <td>481974.0</td>\n      <td>E01027630</td>\n      <td>E02005757</td>\n      <td>D2</td>\n      <td>E12000003</td>\n      <td>E07000164</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>6572</th>\n      <td>YO73RP</td>\n      <td>2018</td>\n      <td>3124.333333</td>\n      <td>Q50</td>\n      <td>03D</td>\n      <td>440259.0</td>\n      <td>475953.0</td>\n      <td>E01027635</td>\n      <td>E02005758</td>\n      <td>E1</td>\n      <td>...</td>\n      <td>1.059750</td>\n      <td>-0.815316</td>\n      <td>440259.0</td>\n      <td>475953.0</td>\n      <td>E01027635</td>\n      <td>E02005758</td>\n      <td>E1</td>\n      <td>E12000003</td>\n      <td>E07000164</td>\n      <td>116</td>\n    </tr>\n    <tr>\n      <th>6573</th>\n      <td>YO84BL</td>\n      <td>2018</td>\n      <td>10717.916667</td>\n      <td>Q50</td>\n      <td>03Q</td>\n      <td>461055.0</td>\n      <td>432478.0</td>\n      <td>E01027908</td>\n      <td>E02005813</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>-0.741733</td>\n      <td>-2.343730</td>\n      <td>461055.0</td>\n      <td>432478.0</td>\n      <td>E01027908</td>\n      <td>E02005813</td>\n      <td>C1</td>\n      <td>E12000003</td>\n      <td>E07000169</td>\n      <td>-94</td>\n    </tr>\n    <tr>\n      <th>6574</th>\n      <td>YO84QH</td>\n      <td>2018</td>\n      <td>16984.666667</td>\n      <td>Q50</td>\n      <td>03Q</td>\n      <td>461500.0</td>\n      <td>432020.0</td>\n      <td>E01027909</td>\n      <td>E02005813</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>-0.524102</td>\n      <td>-1.586530</td>\n      <td>461500.0</td>\n      <td>432020.0</td>\n      <td>E01027909</td>\n      <td>E02005813</td>\n      <td>C1</td>\n      <td>E12000003</td>\n      <td>E07000169</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>YO89AJ</td>\n      <td>2018</td>\n      <td>15926.083333</td>\n      <td>Q50</td>\n      <td>03Q</td>\n      <td>460694.0</td>\n      <td>431348.0</td>\n      <td>E01027882</td>\n      <td>E02005815</td>\n      <td>C1</td>\n      <td>...</td>\n      <td>0.053301</td>\n      <td>-1.301268</td>\n      <td>460694.0</td>\n      <td>431348.0</td>\n      <td>E01027882</td>\n      <td>E02005815</td>\n      <td>C1</td>\n      <td>E12000003</td>\n      <td>E07000169</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n<p>6576 rows Ã— 42 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'oseast1m'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3079\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3080\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'oseast1m'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-54-8b1510f8eede>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Check for missing postcodes\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdata_temp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdata_temp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'oseast1m'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misnull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'pcstrip'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue_counts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3022\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3024\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3025\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3080\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3082\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3084\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'oseast1m'"
     ]
    }
   ],
   "source": [
    "# Check for missing postcodes\n",
    "data_temp[data_temp['oseast1m'].isnull()]['pcstrip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Missing Postcodes - appear to be typos.\n",
    "new_pcs = {'DL154SB': 'DL54SB', 'WR59QT':'WR52QT', 'DN18QN':'DN48QN', 'HU32AE':'HU34AE','GL10QD': 'GL13NN',\n",
    "           'ME122TZ':'ME102TZ', 'BS378NG':'BS374NG', 'CV115PO':'CV115PQ', 'TW152EA':'TW153EA', 'EN24EJ':'EN80BX', \n",
    "           'YO302JS':'YO306JA','L62UN':'L67UN', 'NG698DB':'NG98DA', 'HA24ES':'HA14ES'}\n",
    "\n",
    "data['pcstrip'] = data['pcstrip'].map(new_pcs).fillna(data['pcstrip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "data = data.merge(pc[['pcstrip','oseast1m','osnrth1m','lsoa11','msoa11','ru11ind','rgn','laua','imd']], \n",
    "                  how = 'left', \n",
    "                  on = 'pcstrip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path + \"/processed_data_with_postcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Postcodes \n",
    "\n",
    "Some Practice Codes have more than one postcode associated with them. Possible reasons for this are:\n",
    "* Practices move to a new location.\n",
    "* Practices are assigned a new postcode but don't physically move.\n",
    "* Practice postcodes are wrongly entered at a particular wave and subsequently fixed.\n",
    "\n",
    "There are 794 practices codes which have more than 1 postcode assigned to them, this is about 7% of unique practices.\n",
    "\n",
    "764 for those practices have 2 postcodes associated with them, while 30 have 3 postcodes.\n",
    "\n",
    "102 of these practices fall within the same LSOA, 669 fall within 2 different LSOAs, and 30 within 3 different LSOAs.\n",
    "\n",
    "We'll ignore this for now - this will require some more advanced cleaning - useful to be aware of though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + '/processed_data_with_postcodes.csv')\n",
    "# Check for 1 postcode per Practice Code\n",
    "pc_prac_counts = data.groupby('PracCode')['pcstrip'].unique().map(len)\n",
    "# 794 practices have more than 1 postcode associated with it.\n",
    "pc_prac_counts[pc_prac_counts > 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice codes with multiple associated postcodes account forc. 7% of the data\n",
    "pc_prac_counts[pc_prac_counts > 1].count()/len(pc_prac_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_prac_counts[pc_prac_counts > 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of these Practices falling within the same LSOA\n",
    "(data[data['PracCode'].isin(pc_prac_counts[pc_prac_counts > 1].index)]\n",
    " .groupby('PracCode')['lsoa11']\n",
    " .unique()\n",
    " .map(len)\n",
    " .value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting the Data\n",
    "\n",
    "## Use only General Practice surgeries\n",
    "\n",
    "Use the 'Patients Registered at a GP Practice' data from: to get GP surgery codes and subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + \"processed_data_with_postcodes.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GP files\n",
    "# gp_path = r\"[CHANGE THIS PATH]\\England\\GP data\\\\\"\n",
    "gp_path = r\"england/GP data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combine = []\n",
    "\n",
    "for file in os.listdir(gp_path):\n",
    "    # read file into pandas\n",
    "    month, year = file[-10:-4].split(\"-\")\n",
    "    \n",
    "    # Deal with different file structures\n",
    "    if (year == '16') or (year == '17' and month == 'jan'):\n",
    "        gp_data = pd.read_csv(gp_path + file)\n",
    "        gp_data['DATE'] = \"01\" + month.upper() + \"20\" + year\n",
    "        gp_data.columns = gp_data.columns.str.upper()\n",
    "        gp_data = gp_data.rename(columns = {'DATE':'Date','GP_PRACTICE_CODE':'PracCode','TOTAL_ALL':'NUMBER_OF_PATIENTS'})\n",
    "        gp_combine.append(gp_data[['Date','PracCode','POSTCODE','NUMBER_OF_PATIENTS']])\n",
    "    else:\n",
    "        gp_data = pd.read_csv(gp_path + file)\n",
    "        gp_data.columns = gp_data.columns.str.upper().str.replace(\" \",\"_\")\n",
    "        gp_data = gp_data.rename(columns = {'EXTRACT_DATE':'Date', 'CODE':'PracCode'})\n",
    "        if year == '17' and month == 'jun':\n",
    "            gp_data['Date'] = \"01\" + month.upper() + \"20\" + year\n",
    "        gp_combine.append(gp_data[['Date','PracCode','POSTCODE','NUMBER_OF_PATIENTS']])\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the gp data together.\n",
    "gp_data = pd.concat(gp_combine, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique codes for GP surgeries and subset the prescribing data according to these codes.\n",
    "gp_ids = gp_data['PracCode'].unique()\n",
    "data = data[data['PracCode'].isin(gp_ids)].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date a datetime variable\n",
    "gp_data['Date'] = pd.to_datetime(gp_data['Date'], format = '%d%b%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date a datetime variable - days are assigned as first day of the month.\n",
    "data['Date'] = pd.to_datetime(data['Date'], format = '%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the basis of date and PracCode - produces some nulls for counts.\n",
    "# It may be possible to predict missing values using a time-series model.\n",
    "data = data.merge(gp_data, how = 'left', on = ['Date','PracCode'])\n",
    "# data = data.merge(gp_data, how = 'left', on = ['PracCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path + \"/processed_data_with_postcodes_GPs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Statistics from Prescribing Counts\n",
    "\n",
    "## Percentages At Postcode Level\n",
    "\n",
    "Aggregate observations to postcodes and compute percentages for 'depression', 'alzheimers', 'blood pressure', 'hypertension', 'diabetes', 'cardiovascular disease', 'insomnia', 'addiction', 'social anxiety', and 'loneliness'.\n",
    "\n",
    "## Outlier Removal\n",
    "\n",
    "Should we remove some GPs on the basis that they have very low/high values which might indicate they are not accessible to the general population, and instead represent specialist services?\n",
    "\n",
    "Currently, we won't do this, but it's an advanced task to look into.\n",
    "\n",
    "## Standardise Percentages\n",
    "\n",
    "Should we standardise within time points, or across them? Or standardise with GP practices or across them?\n",
    "\n",
    "Can't standardize within GPs, as can't then compare between GPs.\n",
    "\n",
    "Can't standardise across GPs within years, as can't then compare between years.\n",
    "\n",
    "Can't standardise across GPs across years, as can't then disambiguate changes to rank order over time.\n",
    "\n",
    "<u> First Step </u>\n",
    "\n",
    "Take the average percentage of disease groups within postcodes annually - this is then an annual summary measure of prescribing by postcode. Aggregation entire depends on desired time frame for analysis.\n",
    "\n",
    "NB, this is a mean of percentages - could also calculate an overall percentage by summing monthly counts by year and dividing through by monthly sum of items.\n",
    "\n",
    "<u> Second Step </u>\n",
    "\n",
    "z-score standardise for earliest year observed across GPs. Store mean and standard deviation.\n",
    "\n",
    "z-score standardise subsequent years wrt baseline mean and standard deviation.\n",
    "\n",
    "OR\n",
    "\n",
    "Use min-max normalisation by year (decile normalisation?). This standardises the different percentages to the same range - although in theory they're comparable anyway...\n",
    "\n",
    "## Aggregation and Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + \"/processed_data_with_postcodes_GPs.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary for aggregation\n",
    "# counts to sum\n",
    "agg_cols = {col : 'sum' for col in drug_data['illness'].unique()}\n",
    "agg_cols['ITEMS'] = 'sum'\n",
    "agg_cols['loneliness'] = 'sum'\n",
    "agg_cols['NUMBER_OF_PATIENTS'] = 'sum'\n",
    "\n",
    "# Other data to preserve\n",
    "for key in ['SHA','PCT','Street','Town','Town2','Postcode','oseast1m', 'osnrth1m',\n",
    "            'lsoa11', 'msoa11','ru11ind', 'rgn', 'laua', 'imd']:\n",
    "    agg_cols[key] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do aggregation and produce counts by postcode by date.\n",
    "data = data.groupby(['pcstrip','Date'], as_index=False).agg(agg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate percentages\n",
    "perc_cols = drug_data['illness'].unique()\n",
    "target_cols = perc_cols + '_perc'\n",
    "\n",
    "# Percentages for discrete illness groups\n",
    "data[target_cols] = data[perc_cols].divide(data['ITEMS'], axis=0) * 100\n",
    "\n",
    "# Overall percentage for loneliness realted disease prescribing\n",
    "data['loneliness_perc'] = data['loneliness'].divide(data['ITEMS'], axis=0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation (z-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly aggregate percentages by postcodes by year.\n",
    "#data['Year'] = data['Date'].dt.year\n",
    "data['Year'] = 2018\n",
    "\n",
    "# Aggregation\n",
    "cols = {'NUMBER_OF_PATIENTS': 'mean', 'SHA':'first', 'PCT':'first', 'oseast1m':'first', 'osnrth1m':'first',\n",
    "        'lsoa11': 'first', 'msoa11': 'first', 'ru11ind': 'first', 'rgn': 'first', 'laua':'first', 'imd': 'first',\n",
    "        'depression_perc': 'mean', 'alzheimers_perc': 'mean', 'blood pressure_perc': 'mean', 'hypertension_perc': 'mean',\n",
    "        'diabeties_perc': 'mean', 'cardiovascular disease_perc': 'mean', 'insomnia_perc': 'mean', 'addiction_perc': 'mean',\n",
    "        'social anxiety_perc': 'mean', 'loneliness_perc': 'mean'}\n",
    "\n",
    "data = data.groupby(['pcstrip','Year'], as_index=False).agg(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean value returns a value broadly in the centre of the distribution of respective disease classes.\n",
    "# Therefore we'll go with an un-truncated arithmetic mean.\n",
    "# Can always revisit this assumption later.\n",
    "\n",
    "per_cols = ['depression_perc', 'alzheimers_perc', 'blood pressure_perc', 'hypertension_perc', \n",
    "            'diabeties_perc', 'cardiovascular disease_perc', 'insomnia_perc', 'addiction_perc',\n",
    "            'social anxiety_perc', 'loneliness_perc']\n",
    "\n",
    "# Get mean and std for baseline (2016)\n",
    "mean_std = data[data['Year'] == 2018][per_cols].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new column names.\n",
    "std_cols = [col[:-4] + 'zscore' for col in per_cols]\n",
    "\n",
    "zscores = []    \n",
    "# z-score standardise for each year by baseline mean and std \n",
    "for year in [2016,2017,2018]:\n",
    "    zscores.append((data.loc[data['Year'] == year, per_cols] - mean_std.loc['mean', per_cols]) / mean_std.loc['std', per_cols])\n",
    "\n",
    "zscores = pd.concat(zscores).sort_index()\n",
    "data[std_cols] = zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot zscores for loneliness\n",
    "f, [ax1, ax2, ax3] = plt.subplots(1,3, figsize = (14,6), sharey = True)\n",
    "\n",
    "# Note that there appears to be increasing variation in lonelines prescribing over time.\n",
    "# These means are comparable as standardised using 2016 mean and std.\n",
    "data[data['Year'] == 2016]['loneliness_zscore'].hist(bins=100, ax = ax1)\n",
    "data[data['Year'] == 2017]['loneliness_zscore'].hist(bins=100, ax = ax2)\n",
    "data[data['Year'] == 2018]['loneliness_zscore'].hist(bins=100, ax = ax3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path + \"/processed_data_with_postcodes_GPs_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Loneliness Variable\n",
    "\n",
    "The actual loneliness variable we work with is the sum of the standardised scores of: depression, alzheimers, hypertension, insomnia, addiction and social anxiety, for each year of interest.\n",
    "\n",
    "This means that the loneliness variable is actually an equally weighted index of the above domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum function ignores NAs\n",
    "data['loneills'] =  data[['depression_zscore', 'alzheimers_zscore', 'hypertension_zscore', 'insomnia_zscore',\n",
    "                          'addiction_zscore','social anxiety_zscore']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot zscores for loneills\n",
    "f, [ax1, ax2, ax3] = plt.subplots(1,3, figsize = (14,6), sharey = True)\n",
    "\n",
    "# Note that there appears to be increasing variation in lonelines prescribing over time.\n",
    "data[data['Year'] == 2016]['loneills'].hist(bins=100, ax = ax1)\n",
    "data[data['Year'] == 2017]['loneills'].hist(bins=100, ax = ax2)\n",
    "data[data['Year'] == 2018]['loneills'].hist(bins=100, ax = ax3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path + \"/final_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}